\documentclass[11pt]{extarticle} %extarticle for fontsizes other than 10, 11 And 12
%\documentclass[11p]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Input header file 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{HeaderfileTexDocs}

%%%%%%%%%%%%%%%%%%% To change the margins and stuff %%%%%%%%%%%%%%%%%%%
\geometry{left=0.8in, right=0.9in, top=0.9in, bottom=0.8in}
%\setlength{\voffset}{0.5in}
%\setlength{\hoffset}{-0.4in}
%\setlength{\textwidth}{7.6in}
%\setlength{\textheight}{10in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Topics for Data Science Internship \& Job Interviews}
\author{Subhrangshu Nandi}
\date{}

\maketitle

\tableofcontents
\newpage

\section{Preprocessing}
\subsection{Principal Component Analysis}
\begin{itemize}
\item Reduce number of predictors
\item Reduce noise
\item Tackle multicollinearity
\item Interpretability could be hard
\item Should use the same PCA in the test and train datasets
\item Outliers should be removed before PCA
\end{itemize}


\section{Prediction}
Stuff

\section{Regression Trees}
\begin{itemize}
\item Easy to interpret
\item Better performance in non-linear settings
\item Could lead to overfitting
\item Hard to estimate uncertainty
\item Gini index: measure of impurity $1 - \sum\limits_{k = 1}^{K} \hat{p}^2_{mk}$. Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. 
\end{itemize}

\subsection{Bagging - Bootstrap aggregating}
\begin{itemize}
\item Resample cases and recalculate predictions; then average or majority votes
\item Reduced variance
\item More useful for nonlinear functions
\end{itemize}

\subsection{Random Forest}
\begin{itemize}
\item Similar to bagging, but here, at each resampling, a subset of the features are included in building the trees.
\item Grow multiple trees and vote
\item Pros: Highly accurate
\item Cons: Slow; Poor interpretability; Overfitting
\end{itemize}

\subsection{Boosting}
\begin{itemize}
\item Take $h_i,\dots, h_k$ classifiers and combine them to one classifier, by weighting $f(x) = \text{sgn}\left(\sum\limits_{t=1}^T \alpha_t h_t(x) \right)$
\item Goal: To minimize error
\item Popular boosting algorithm: adaboost
\item Boosting with trees; model based boosting; boosting with generalized additive models.
\end{itemize}

\subsection{Ensemble methods}
\begin{itemize}
\item Combine classifiers from different methods by averaging/voting
\item Improves accuracy; Reduces interpretability; 

\end{itemize}
\section{Vocabulary}
\begin{itemize}
\item Generalization error: Out-of-sample error
\item Resubstitution error: In-sample error
\item ROC: Receiver operating characteristics
\end{itemize}

\section{Miscellaneous Notes}
\begin{itemize}
\item Types of errors: Sensitivity; Specificity; True positives; False positives; Positive/Negative predicted value (conditional probability)
\item Common error measures: Mean squared error; median absolute deviation; sensitivity; specificity; accuracy; concordance;
\item Use R package {\emph{caret}} for most machine learning purposes. R package {\emph{kernlab}} has a lot of data science examples. 
\item Machine learning algorithm steps: \\ Question $->$ Data $->$ Features $->$ Algorithms $->$ Prediction/Parameters $->$ Evaluation
\item Prediction is about accurace tradeoffs: in favor of simplicity, interpretability, speed, scalability
\item Training (60\%), Testing (20\%), Validation (20\%); OR Training (60\%), Testing (40\%)
\item Bootstrap could underestimate the error rate. Use the 0.632 Bootstrap to adjust for the error. Refer to Efron\_Tibshirani\_1997\_JASA.
\item If you transform the variables on the training set, use the exact same transformations on the test set.
\end{itemize}
\end{document}

