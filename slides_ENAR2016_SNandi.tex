%\documentclass[10pt,dvipsnames,table, handout]{beamer} % To printout the slides without the animations
\documentclass[10pt,dvipsnames,table]{beamer} 
%\usetheme{Luebeck} 
%\usetheme{Madrid} 
%\usetheme{Marburg} 
%\usetheme{Warsaw} 
\usetheme{CambridgeUS}
%\setbeamercolor{structure}{fg=cyan!90!white}
%\setbeamercolor{normal text}{fg=white, bg=black}
\setbeamercolor{block title}{bg=red!80,fg=white}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Input header file 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{HeaderfileTexSlides}

\logo{\includegraphics[scale=0.4]{uwlogo_web_sm_fl_wht.png}}
%\logo{\includegraphics[width=\beamer@sidebarwidth,height=\beamer@headheight]{uwlogo_web_sm_fl_wht.png}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE PAGE 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\title[Functional outliers]{Detecting outliers in functional data}
\author[S. Nandi]{Subhrangshu Nandi \inst{1} \and Chengyue Wu \inst{2} \and Alicia Nieto-Reyes \inst{3} \and Michael A. Newton \inst{1}} 
\institute[UW Madison]{\inst{1} University of Wisconsin-Madison \and %
                      \inst{2} University of Science and Technology of China \and %
                      \inst{3} Universidad de Cantabria}

%\author[S. Nandi]{Subhrangshu Nandi}
%\institute[UW Madison]{
%Department of Statistics \\
%University of Wisconsin-Madison \\
%\vspace{1cm}
%ENAR 2016}
\date{March 8, 2016}

\begin{document}
\setlength{\baselineskip}{16truept}
\setbeamertemplate{logo}{}

\frame{\maketitle}

%%%%%%%%%%%%%% Slide 1 %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Objective of this talk}
{\Large{
\begin{enumerate}
\item Explore some functional data depth concepts
\vspace{0.5cm}
\item Use them to detect outliers (distant realizations) in functional data
\vspace{0.5cm}
\item Apply them to an image-processing data of cells
\end{enumerate}
}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{What the data looks like}
\vspace{-0.5cm}
\begin{figure}
\includegraphics[scale = 0.3, page = 2]{Plots/chr13_frag66.pdf}
\end{figure}

\vspace{-0.5cm}
\begin{block}{Goal of the project}
To estimate a consensus profile (some measure of average) of these realizations (curves)
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Outliers in univariate data}
\vspace{-0.5cm}
\begin{figure}[t]
\centering
\includegraphics[scale = 0.45]{Outliers_Univ/Slide1.PNG}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Concept of Depth in univariate data}
\begin{itemize}
\item For a set of $n$ distinct points $x_1, x_2, \dots, x_n \in \Real$, the depth of a point $x \in \Real$ is
\[ d(x) = \min(\sum \limits_{i = 1}^n \Ind\{ x_i \leq x\}, \sum \limits_{i = 1}^n \Ind\{ x_i \geq x\}) \]
\vspace{0.5cm}
\item {\bf{Median}} is the point of maximum depth. 
\[ d(\text{Median}\left(x_1, \dots, x_n)\right) = \floor{\frac{n + 1}{2}} \]
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Outliers in bivariate data}
\vspace{-0.5cm}
\begin{figure}[t]
\centering
\includegraphics[scale = 0.3]{Plots/Plot2.jpg}
\end{figure}
\vspace{-0.5cm}
How to detect an outlier (or distant observation) in a bivariate data?
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Outliers in bivariate data}
\vspace{-0.5cm}
\begin{figure}[t]
\centering
\includegraphics[scale = 0.3]{Outliers_Univ/Slide2.PNG}
\end{figure}
\vspace{-0.5cm}
Need a measure of multivariate depth in $\Real ^ p$ and depth of ``functional'' data
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Functional data framework}
\begin{itemize}
\item {\emph{A functional random variable}}: $\Y$ takes values in an infinite-dimensional space. An observation $Y$ of $\Y$ is called a functional data.
\item {\emph{A functional data}} $Y_1, \dots, Y_n$ is an observation of $n$ functional variables $\Y_1, \dots, \Y_n$, where $\Y_i \stackrel{iid}{\sim} \Y$. $Y_i = {y_{i,1}, \dots, y_{i,p}}$ are $p$ discretely observed points of the function $\Y_i$
\item The abscissa will be denoted by $x$, where the observations $y_{i,1}, \dots, y_{i,p}$ are made at $x_1 \dots x_p$
\item In our data, each curve will be denoted by $Y_i, \ i = 1, \dots n$
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Functional data depth}
\LARGE
\begin{enumerate}
\item The Fraiman and Muniz depth
\vspace{0.25in}
\item The h-modal depth
\vspace{0.25in}
\item The random projection depth
\vspace{0.25in}
\item The half-space Tukey depth
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Fraiman-Muniz depth}
\small
\begin{itemize}
\item The empirical c.d.f. of the values of the curves $y_1(x), \dots , y_n(x)$ at a given position $x \in [a, b]$
\[ F_{n,x} (y_i (x)) = \frac{1}{n}\sum\limits_{k=1}^{n} \Ind\{ y_k(x) \leq y_i(x)\}\]
\pause
\item The univariate depth of a point $y_i(x)$ is given by
\[ D_n(y_i(x)) = 1 - \left| \frac{1}{2} - F_{n,x} (y_i (x)) \right| \]
Note the $D_n(y_i(x)) \in [0, 1]$
\pause
\item Fraiman and Muniz functional depth (FMD), of a curve $y_i$ with respect the set $y_1(x), \dots , y_n(x)$ is given by 
\[ \text{FMD}_n(y_i) = \int\limits_a^b D_n(y_i(x)) dx \]
Note: Higher values of FMD implies deeper curve; lower values of FMD implies more distant from the deepest curve
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{The h-modal depth}
The h-modal depth of a curve $y_i$ with respect the set $y_1(x), \dots , y_n(x)$ is given by
\[ \text{MD}_n(y_i, h) = \sum\limits_{k = 1}^n K\left( \frac{\|y_i - y_k \|}{h} \right),\]
where, \begin{itemize}
\item $\|\cdot\| = L^2 \text{ or } L^{\infty}$ norm (for example)
\item $K(\cdot): \Real^{+} \rightarrow \Real^{+}$ is a kernel function, for example, $K(x) = \frac{2}{\sqrt{2\pi}} \text{exp}\left(-\frac{x^2}{2}\right), t > 0$, the truncated Gaussian kernel;
\item $h$ reasonable bandwidth, for example, $15^{th}$ percentile of the empirical d.f. of $\{ \|y_i - y_k \|, i, k = 1, \dots n \}$
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{The random projection depth}
\begin{itemize}
\item When a sample $y_1, \dots, y_n$ of curves is projected on a random direction $a, a \indep y_i$, with the operation $ \langle a, \mathbf{y} \rangle$ then sample depth of $y_i$ is the rank (univariate depth) of $ \langle a, y_i \rangle$
\pause
\item For example, if $y_i \in \text{ Hilbert space } L^2[0,1]$, then the projection is defined by its standard inner product 
\[ \langle a, y \rangle = \int\limits_0^1 a(x) y(x) dx\]
\pause
\item The random projection depth of a curve $y_i$ with respect the set $y_1(x), \dots , y_n(x)$ is defined as
\[ \text{RPD}_n(y_i) = \frac{1}{P} \sum\limits_{j = 1}^P D_n(\langle a_j, y_i \rangle)\]
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{The half-space Tukey depth}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Depth and Outliers}
\Large
\begin{block}{Outliers?}
How do we detect outliers using these data depth definitions?
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Detect Outliers using depth}
For a sample of curves $y_1, \dots , y_n$
\begin{enumerate}
\item[1] Estimate functional depth $D_n(y_1), \dots , D_n(y_n)$ using any of the above depth functions FMD, MD, RPD, RTD
\item[2] For a cutoff $C$, if $D_n(y_j) \leq C$, for any $j \in 1, \dots, n$, consider $y_j$ as outliers. Remove them from the sample
\item[3] Repeat steps [1] and [2] until no outliers are detected
\end{enumerate}
\vspace{1cm}
\pause
\begin{block}{What is $C$?}
Do we know the distributions of the functional data depth estimators?
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{How do we estmate Cutoff $C$?}
\Large
\begin{block}{Obtain $C$ by nonparametric bootstrap procedure}
We need to select $C$ such that in the absence of outliers, \[ \Prob\left( D_n(y_i) \leq C\right) = 0.01,\ \ i = 1, \dots, n \]
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Bootstrap procedure for $C$}
\begin{enumerate}
%\small
\item[1] Obtain the functional depths $D_n(y_1), \dots, D_n(y_n)$, using any depth function
\pause
\item[2] Obtain $B$ standard bootstrap samples from the curves in which each original curve is sampled with a probability proportional to its depth. Let $y_i^b, i = 1, \dots, n$ and $b = 1, \dots, B$, be these samples.
\pause
\item[3] For each bootstrap, set $b = 1, \dots, B$, obtain $C_b$ as the $\alpha^{th}$ empirical percentile of the distribution of the depths, $D(y_i^b(x)), i = 1, \dots, n$
\pause
\item[4] Take $C = \text{median}(C_b), b = 1, \dots, B$ 
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Application to image processing data}
\Large
\begin{block}{Comparison of outliers by different depth functions}
\begin{enumerate}
\item Detect outliers by image processing
\item Detect outliers by different data depths
\item Choose technique that maximizes power
\end{enumerate}
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Case study: application to sample of 23 curves}
\vspace{-0.5cm}
\begin{figure}[t]
\includegraphics[scale = 0.23, page = 3]{Plots/OutlierComparisons_66.pdf}
\hspace{0.5cm}
\pause
\includegraphics[scale = 0.23, page = 4]{Plots/OutlierComparisons_66.pdf} \\
\includegraphics[scale = 0.23, page = 6]{Plots/chr13_frag66.pdf}
\pause
\hspace{0.5cm}
\pause
\includegraphics[scale = 0.23, page = 7]{Plots/chr13_frag66.pdf}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Results}
\begin{itemize}
\item Outlier comparison applied to 144 samples, each with at least 20 curves
\item Images of each curve analyzed, and tagged as ``Poor'' or ``Good'' quality
\item Curves were normalized by its median, smoothed by B-spline ($4^{th}$ order)
\item Outliers of all samples were estimated by four different data-depth methods
\item Power of outlier detection was estimated
\end{itemize}
\pause
Total image processed outliers: 373 (11.2\% of all samples) \\
% latex table generated in R 3.2.2 by xtable 1.8-0 package
% Sun Mar  6 00:30:24 2016
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
  \hline
  Depth function & Outliers & Power \\ 
  \hline
  FMD & 34 & 9.12 \\ 
  h-Mode & 64 & 17.16 \\ 
  RTukey & 70 & 18.77 \\ 
  RProj & 73 & 19.57 \\ 
  Union & 121 & 32.44 \\ 
  \hline
  \hline
\end{tabular}
\end{table}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Slide x %%%%%%%%%%%%%%
\begin{frame}
\frametitle{Summary}
\begin{itemize}
%\small
\item Outlier detection in functional data significantly more challenging
\item Different techniques have different strengths and weaknesses
\item Outliers should be further investigated, instead of being discarded from the data
\item Outlier detection may not be robust to choice of smoothing techniques
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

