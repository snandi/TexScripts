%\documentclass[10pt,dvipsnames,table, handout]{beamer} % To printout the slides without the animations
\documentclass[10pt,dvipsnames,table]{beamer} 
%\usetheme{Luebeck} 
\usetheme{Madrid} 
%\usetheme{Marburg} 
\setbeamercolor{structure}{fg=cyan!90!white}
%\setbeamercolor{normal text}{fg=white, bg=black}

%%%%%%%%%%%%%%%%%%%%%%%% Packages %%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amscd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsxtra}
\usepackage{bbold}
%\usepackage{bigints}
\usepackage{color}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage[mathscr]{eucal}
%\usepackage{fancyhdr}
\usepackage{float}
%\usepackage{fullpage} %% Dont use this for beamer presentations
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{lscape}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{natbib}
\usepackage{pdfpages}
\usepackage{verbatim}
\usepackage{wrapfig}
\usepackage{xargs}
\DeclareGraphicsExtensions{.pdf,.png,.jpg, .jpeg}

%%%%%%%%%%%%%%%%%%%%%%%% Commands %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Sup}{\textsuperscript}
\newcommand{\Exp}{\mathds{E}}
\newcommand{\Prob}{\mathds{P}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\Ind}{\mathds{1}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bes}{\begin{equation*}}
\newcommand{\ees}{\end{equation*}}
\newcommand{\union}{\bigcup}
\newcommand{\intersect}{\bigcap}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\betahat}{\hat{\beta}}
\newcommand{\Yhat}{\widehat{Y}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\Xhat}{\widehat{X}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\E}[1]{\operatorname{E}\left[ #1 \right]}
%\newcommand{\Var}[1]{\operatorname{Var}\left( #1 \right)}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}[2]{\operatorname{Cov}\left( #1,#2 \right)}
\newcommand{\N}[2][1=\mu, 2=\sigma^2]{\operatorname{N}\left( #1,#2 \right)}
\newcommand{\bp}[1]{\left( #1 \right)}
\newcommand{\bsb}[1]{\left[ #1 \right]}
\newcommand{\bcb}[1]{\left\{ #1 \right\}}
\newcommand*{\permcomb}[4][0mu]{{{}^{#3}\mkern#1#2_{#4}}}
\newcommand*{\perm}[1][-3mu]{\permcomb[#1]{P}}
\newcommand*{\comb}[1][-1mu]{\permcomb[#1]{C}}

%%%%%%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%%%%%%
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\title[Two-sample tests in functional data analysis]{TWO-SAMPLE TESTS IN FUNCTIONAL DATA ANALYSIS STARTING FROM DISCRETE DATA \\ Peter Hall and Ingrid Van Keilegom}
\author{Subhrangshu Nandi}
\institute[Stat 809]{Stat 809, Fall 2014 \\
  Department of Statistics \\
 University of Wisconsin-Madison}
\date{November 24, 2014}

\begin{document}
\setlength{\baselineskip}{16truept}
\frame{\maketitle}

%%%%%%%%%%%% Slide 1 %%%%%%%%%%%%
%\begin{frame}
%\frametitle{Outline}
%\begin{itemize}
% \item Introduction to functional data
% \item Motivation of potential problem
% \item Objective: Minimize effets of smoothing in two sample tests
% \item Problem definition and two sample test
% \item Some theoretical results
% \item Computational results
% \item Summary
%\end{itemize}
%\end{frame}

%%%%%%%%%%%% Slide 2 %%%%%%%%%%%%
\begin{frame}
\frametitle{What is functional data?}
Functional data describe (nearly) a process that changes smoothly, and continuously, temporally or spatially or both.\\
What are the features of functional data? 

\begin{columns}
\begin{column}{5cm}
\includegraphics[scale=0.32]{Plot1.jpeg}
\end{column}
\begin{column}{5cm}
\begin{itemize}
%\item  Quantity
\item  Frequency
\begin{itemize}
\item \textcolor{green}{equally spaced intervals}
\item \textcolor{red}{irregular intervals}
\end{itemize}
\item  Similar trends
\item  (Most importantly) {\bf{Smoothness}}
\end{itemize}
%\vspace{2cm}
\end{column}
\end{columns}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%% Slide 2a %%%%%%%%%%%%
\begin{frame}
\frametitle{What is functional data (contd)?}
\begin{columns}
\begin{column}{5cm}
\includegraphics[scale=0.28]{Plot1.jpeg}
\end{column}
\begin{column}{5cm}
\begin{itemize}
\item  Measurements of smooth processes over time
\item  Usually do not want to make parametric assumptions about the processes
\item  Often have multiple measurements of the same process
%\item  Interested in describing the variation of processes
\end{itemize}
\end{column}
\end{columns}
\end{frame}

%%%%%%%%%%%% Slide 3 %%%%%%%%%%%%
\begin{frame}
\frametitle{Examples of functional data}
\begin{columns}
\begin{column}{6cm}
\includegraphics<2>[scale=0.13]{Philly_1876.jpeg}
\includegraphics<4>[scale=0.18]{ECGSmoothedData.jpg}
\end{column}
\begin{column}{4cm}
\begin{itemize}
\item <1-> economics/marketing: macro-trends, futures markets
\item <2-> environmental monitoring: weather, pollution, traffic
\item <3-> biology: animal behavior (whale songs, fly egg-laying, etc)
\item <4-> medical monitoring: EEG, ECG, fMRI, blood pressure
\end{itemize}
\end{column}
\end{columns}
\end{frame}

%%%%%%%%%%%% Slide 4 %%%%%%%%%%%%
\begin{frame}
\frametitle{Motivation}
\begin{block}{Goal}
To compare two samples of functional data and assert if they are from the same distribution or not
\end{block}
\begin{itemize}
\item  Functional data are invariably recorded discretely
\item  Widespread use of pre-process (smoothing) for functional data
\item  Effect of different smoothing parameters on this two-sample test?
\end{itemize}
\end{frame}

%%%%%%%%%%%% Slide 5 %%%%%%%%%%%%
\begin{frame}
\frametitle{Problem definition}
$U_{ij} = X_i(S_{ij}) + \delta_{ij},\ \ 1 \leq i \leq m,\ \ 1 \leq j \leq m_i,$ \\
$V_{ij} = Y_i(T_{ij}) + \epsilon_{ij},\ \ 1 \leq i \leq n,\ \ 1 \leq j \leq n_i,$ \\

\begin{itemize}
\item
$X_1,\ X_2,\dots$ are identically distributed as X
\item
$Y_1,\ Y_2,\dots$ are identically distributed as Y
\item
$X$ and $Y$ are both random functions, defined on the interval $\I = [0, 1]$
\item
$\delta_{ij}$ are identically distributed as $\delta$; $\epsilon_{ij}$ are identically distributed as $\epsilon$
\item
$\delta_{ij}$ and $\epsilon_{ij}$ have zero means and uniformly bounded variances
\item
The quantities $X_{i_1},\ Y_{i_2},\ S_{i_1j_1},\ T_{i_2j_2}, \delta_{i_1}$ and $\epsilon_{i_2}$ are all independent of each other.
\end{itemize}

\begin{block}{Null Hypothesis}
$H_0: $ Distributions of $X$ and $Y$ are identical. \\ $\Leftrightarrow$ \\ $F_X(z) = F_Y(z), \forall $ continuous functions $z$, if $X,\ Y$ continuous a.s.
\end{block}
\end{frame}

%%%%%%%%%%%% Slide 6 %%%%%%%%%%%%
\begin{frame}
\frametitle{Estimating $X_i$ and $Y_i$}
Two options:
\begin{itemize}
\item  Local polynomial methods
\item  Smoothing methods, like spline fitting
\end{itemize}
 Recall: $U_{ij} = X_i(S_{ij}) + \delta_{ij}\ \ 1 \leq i \leq m,\ \ 1 \leq j \leq m_i$\\
 To estimate $\hat{X_i}$, estimate the coefficients of the local polynomial fit that minimizes:
\[\sum _{j=1}^{m_i}\left\{U_{ij} - \sum_{k=0}^{2r - 1} a_k(S_{ij} - t)^k\right\}^2 K\left(\frac{t - S_{ij}}{h_{X_i}} \right) \]

$h_{X_i}$ is the bandwidth and $K(\cdot)$ is a kernel function. \\

Assuming $X_i$ has $2(r + 1)$ bounded derivatives, and choosing $h_{X_i} = m_i^{-\frac{1}{2r + 1}}$ \\
The MSE, $\Exp(\hat{X_i} - X_i)^2 = o_p(m_i^{-\frac{2r}{2r+3}}) $
\end{frame}

%%%%%%%%%%%% Slide 7 %%%%%%%%%%%%
\begin{frame}
\frametitle{Hypothesis Testing Methodology}
Recall: $H_0: F_X(t) = F_Y(t)$ \\
 Use $\hat{X_i}$, $\hat{Y_i}$ to estimate the empirical cdfs: 
\[ \hat{F}_X(z) = \frac{1}{m}\sum_{i=1}^{m} \Ind(\hat{X_i} \leq z), \ \hat{F}_Y(z) = \frac{1}{n}\sum_{i=1}^{n} \Ind(\hat{Y_i} \leq z)\]

\begin{block}{Test Statistic}
\begin{itemize}
%\item Cram\'{e}r-von Mises type: $\bigintss \left\{\hat{F}_X(z) - \hat{F}_Y(z) \right\}^2 \mu(dz)$ 
\item Cram\'{e}r-von Mises type: $\hat{T} = \int \{\hat{F}_X(z) - \hat{F}_Y(z) \}^2 \mu(dz)$ 
\item Kolmogorov Smirnoff type: $\hat{T}' = \sup_z|\hat{F}_X(z) - \hat{F}_Y(z)|$
\end{itemize}
Typically, $\hat{T}$ is preferred as it produces tests of more power.
\end{block}
\end{frame}

%%%%%%%%%%%% Slide 7 %%%%%%%%%%%%
\begin{frame}
\frametitle{Bootstrap Callibration}
\begin{itemize}
\item Pool estimators $X_i$, $Y_i$ together: $\mathcal{Z} = \{Z_1, Z_2, \dots, Z_{m+n} \} = \{X_1,\dots, X_m \} \cup \{Y_1,\dots, Y_n \}$
\item Randomly resample, with replacement, $\{X_1^*,\dots, X_m^* \}$ and $\{Y_1^*,\dots, Y_n^* \}$
\item Compute bootstrap empirical cdf: \[\bar{F}^*_X(z) = \frac{1}{m}\sum_{i=1}^{m} \Ind(X^*_i \leq z), \ \bar{F}^*_Y(z) = \frac{1}{n}\sum_{i=1}^{n} \Ind(Y_i^* \leq z)\]
\item Estimate $\bar{T}^* = \int \{\bar{F}^*_X(z) - \bar{F}^*_Y(z) \}^2 \mu(dz)$
\item By repeated resampling, compute $\hat{t}_{\alpha}$ from $\Prob(\bar{T}^* \geq \hat{t}_{\alpha} | \mathcal{Z}) = \alpha$
\item Reject $H_0$, if $\hat{T} > \hat{t}_{\alpha}$
\end{itemize}
\end{frame}

%%%%%%%%%%%% Slide 8 %%%%%%%%%%%%
\begin{frame}
\frametitle{Concerns - Power, Bias?}
Some potential difficulties with this testing procedure:
\begin{itemize}
\item  Regardless of smoothness of data, $\Exp(\bar{F}^*_X(z) - \bar{F}^*_Y(z) | \mathcal{Z}) = 0$, but, even if $H_0$ is true, $\Exp(\hat{F}_X(z) - \hat{F}_Y(z))$ will not vanish - reason: different sampling times, different bandwidth.
\item  Test statistic $\hat{T}$ suffers from biases not reflected in $\bar{T}^*$, thus causing ``loss of power''.  
\end{itemize}

Define the difference between the two statistics: 
\[D(z) = \Exp\{\bar{F}_X(z) - \bar{F}_Y(z)\} - \Exp\{\hat{F}_X(z) - \hat{F}_Y(z)\} \]
\[\widetilde{T} = \int \{\bar{F}_X(z) - \bar{F}_Y(z) - D(z) \}^2 \mu(dz) \]
\end{frame}

%%%%%%%%%%%% Slide 8 %%%%%%%%%%%%
\begin{frame}
\frametitle{Results}
\begin{theorem}
\[|\sqrt{\hat{T}} - \sqrt{\widetilde{T}}| = o_p(\frac{1}{\sqrt{m}} + \frac{1}{\sqrt{n}})\]
In words: $\hat{T}$ can be closely approximated by $\widetilde{T}$\\
$D(z)$ captures all the main issues that will affect the power and level accuracy of the statistic $\hat{T}$
\end{theorem}

\begin{block}{Equality of bandwidth}
For functions $z$ that are sufficiently smooth, and for each $\eta > 0$,
\[\Prob(\hat{X_i} \leq z) = \Prob(X_i \leq z) + O\{h_{X_i}^{2r - \eta} + (m_ih_{X_i})^{\eta - 1} \}\]
\[\Prob(\hat{Y_i} \leq z) = \Prob(Y_i \leq z) + O\{h_{Y_i}^{2r - \eta} + (n_ih_{Y_i})^{\eta - 1} \}\]
For similar subsample sizes ($m_i$, $n_i$), taking the bandwidths $h = h_{X_i} = h_{Y_j}$ reduces the bias. 
 In fact, if $m=n=\nu$, then $h=\nu^{-1/(2r+1)}$ results in \[D(z) = O(\nu^{-2r/(2r+1)})\]. 
\end{block}
\end{frame}

%%%%%%%%%%%% Slide 9 %%%%%%%%%%%%
\begin{frame}
\frametitle{Results (cont)}
For example, for local-linear fit: 
\begin{itemize}
\item  $r = 1,\ \implies h = \nu^{-1/5}$ \hspace{1in} ($\nu = m = n$, is the sample size)
\item  Under $H_0$, $\Prob(\hat{X_i} \leq z) - \Prob(\hat{Y_i} \leq z) = O\{h^{4 - \eta} + (\nu h)^{\eta - 1}\}$ 
\item  $\implies D(z)$ is close to zero
\item  $\implies \hat{T}$ is well approximated by $\bar{T}$
\item  $\implies$ that the bootstrap test is unlikely to reject $H_0$ simply because of poor choice of smoothing parameters
\end{itemize}

\begin{block}{Power properties}
If $m$ and $n$ vary such that $\frac{m}{n} \rightarrow \rho \in (0, \infty),$ as $m \rightarrow \infty$, if $F_Y(z)$ is fixed and $F_X(z) = F_Y(z) + \sqrt{m}c\delta(z)$, then it can be shown that \[\lim_{c \rightarrow \infty} \liminf\limits_{m \rightarrow \infty} \Prob_x(\bar{T} > \hat{t}_{\alpha}) = 1\]
Notice this holds irrespective of subsample sizes.
\end{block}
\end{frame}

%%%%%%%%%%%% Slide 10 %%%%%%%%%%%%
%\begin{frame}
%\frametitle{Simulation Results - Effect of subsampling size}
%\begin{columns}
%\begin{column}{5cm}
%\includegraphics[scale=0.42]{Plot2.jpeg}
%\end{column}
%\begin{column}{.2\textwidth}
%{\bf{Main results}}\\
%\textcolor{green}{(1) Value of the subsample sizes $m_1$ and $n_1$ has limited impact on the power}\\ 
%\vspace{3cm}
%\textcolor{green}{(2) Identical bandwidths lead to higher power} \\
%\end{column}
%\end{columns}
%\end{frame}

%%%%%%%%%%%% Slide 10a %%%%%%%%%%%%
\begin{frame}
\frametitle{Simulation Setup}
\begin{itemize}
\item
$S_{ij} (1 \leq i \leq m; 1 \leq j \leq m_i)$ are i.i.d. $U[0, 1]$
\item
$T_{ij} (1 \leq i \leq n; 1 \leq j \leq n_i)$ are i.i.d. with density $2 − b + 2(b − 1)t$ for $0 \leq t \leq 1$ and $0 < b < 2$. Note that this last density reduces to the uniform
density when $b = 1$
\item
$\delta_{ij} \thicksim \mathcal{N}(0, 0.1); \ \ \epsilon_{ij} \thicksim \mathcal{N}(0, 0.3);$
\item
Suppose $X_1, \dots,X_m$ are identically distributed as $X$, where $X(t) = \sum_{k \geq 1} c_k N_{kX} \psi_k(t),$, where, $c_k = e^{-k/2}, N_{kX}$ are i.i.d. $\mathcal{N}(0,1)$ and $\psi _k(t) = \sqrt{2} sin\{(k −1)\pi t\}\Ind(k > 1)$ and $\psi _1 \equiv 1$ are orthonormal basis functions.
\item
Similarly, suppose $Y_1, \dots,Y_m$ are identically distributed as $Y$, where $Y(t) = \sum_{k \geq 1} c_k N_{kY_1}\psi_k(t) + a\sum_{k \geq 1} b_k N_{kY_2}\psi ^* _k(t)$
\end{itemize}
\end{frame}

%%%%%%%%%%%% Slide 10a %%%%%%%%%%%%
\begin{frame}
\frametitle{Simulation Setup (cont)}
\begin{itemize}
%\item
%Simulation results are based on 500 samples
\item
Critical values of the test are obtained from 250 bootstrap samples
\item
Functions $X(t)$ and $Y(t)$ are estimated by means of local-linear smoothing
\item
The bandwidth is chosen to minimise the cross-validation type criterion, of both $X_i$ and $Y_i$
\item 
{\bf{Study 1}}: Different sample sizes are tested: $m = n = 15, 25, 50$; Subsample sizes are tested for $m_1 = n_1 = 20, 100$, for each of the sample sizes. 
\item 
{\bf{Study 2}}: Same sample sizes $m = n = 15, 25, 50$, but different subsample sizes: $m_1 = 20, n_1 = 100$
\end{itemize}
\end{frame}

%%%%%%%%%%%% Slide 10 %%%%%%%%%%%%
\begin{frame}
\frametitle{Simulation Results - Effect of subsampling size}
\textcolor{yellow}{(1) Value of the subsample sizes $m_1$ and $n_1$ has limited impact on the power}\\ 

%(2) Identical bandwidths lead to higher power \\
\includegraphics[scale=0.45]{Plot2.jpeg}
\end{frame}

%%%%%%%%%%%% Slide 11 %%%%%%%%%%%%
\begin{frame}
\frametitle{Simulation Results - Bandwidth}
\textcolor{yellow}{(2) Identical bandwidths lead to higher power}

\includegraphics[scale=0.45]{Plot3.jpeg}
\end{frame}

%%%%%%%%%%%% Slide 12 %%%%%%%%%%%%
%\begin{frame}
%\frametitle{Application to Real data}

%\end{frame}



% \begin{frame}
% \begin{theorem}
% (i) Let $m=1,\ldots,s_1.$ We consider two runs $C_i,C_j,i\neq j$ in the subdesign $C^{m}.$ Then for $k=1,\ldots,d$ and $x,y\in \textbf{Z}_{s_1^2}$ 
% $$
% Pr(C_{ik}=x, C_{jk}=y) =\begin{cases}
% \frac{1}{s_1^3(s_1-1)},& \ceil{\frac{x}{s_1}}\neq\ceil{\frac{y}{s_1}}\\
% 0,&o.w.
% \end{cases}
% $$
% (ii) The joint probability mass function for $c_{m_1,ik},c_{m_2,jk},m_1\neq m_2$ is 
% $$
% Pr(c_{m_1,ik}=x,c_{m_2,jk}=y) =\begin{cases}
% \frac{1}{s_1^4},& \ceil{\frac{x}{s_1}}\neq\ceil{\frac{y}{s_1}}\\
% \frac{1}{s_1^3(s_1-1)},&\ceil{\frac{x}{s_1}}=\ceil{\frac{y}{s_1}},x\neq y\\
% 0,&o.w.
% \end{cases}
% $$
% \end{theorem}
% \end{frame}

% \begin{frame}
% \begin{theorem}
% Consider two row $C_i,C_j,i\neq j$ in the slice $C^{(m)}.$ Then for $k,l=1,\ldots,d$ and $k\neq l$
% $$ Pr(c_{ik},c_{il},c_{jk},c_{jl}) =\begin{cases}
% \frac{1}{s_1^2s_2(s_2-1)(s_1-1)},& (i,j)\in H_1\\
% \frac{1}{s_1^2s_2^2(s_1-1)},&(i,j)\in H_2.
% \end{cases}
% $$
% \end{theorem}
% \begin{theorem}
% \begin{itemize}
% \item $Var(\hat{\mu}_m) = s_1^{-1}\sum_{|u|\geq3}Var\{f_{m,u}(X)\} + o(s_1^{-1})$
% \item $Var(\hat{\mu}) = s_1^{-2}\sum_{|u|\geq3}Var\{f_u(X)\} + o(s_1^{-2})$
% \end{itemize}
% \end{theorem}
% \end{frame}
% %%%%%%%%%%%%%


\end{document}

